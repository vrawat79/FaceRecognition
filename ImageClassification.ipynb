{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3DR-eO17geWu"
   },
   "source": [
    "# Face Recognition: Binary Image Classification Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process training set to extract faces.. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-24 06:56:59.342613: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-10-24 06:56:59.705579: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 12 calls to <function Model.make_predict_function.<locals>.predict_function at 0x18914f670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:5 out of the last 26 calls to <function Model.make_predict_function.<locals>.predict_function at 0x188af0ca0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "# do image extraction i.e., just extract faces to increase accuracy.. this will exclude clothing \n",
    "\n",
    "# extract and save each detected face in a photograph\n",
    "from matplotlib import pyplot\n",
    "from mtcnn.mtcnn import MTCNN\n",
    " \n",
    "# save each face separately\n",
    "def extract_faces(filename, result_list):\n",
    "    # load the image\n",
    "    data = pyplot.imread(filename)\n",
    "    # plot each face as a subplot\n",
    "    for i in range(len(result_list)):\n",
    "        confidence = result_list[i]['confidence']        \n",
    "        # get coordinates\n",
    "        x1, y1, width, height = result_list[i]['box']\n",
    "        x2, y2 = x1 + width, y1 + height\n",
    "        if confidence >= 0.95:\n",
    "                pyplot.imsave('output/'+ filename, data[y1:y2, x1:x2])        \n",
    "\n",
    "import glob\n",
    "images = glob.glob(\"dataset/training_set/*/*.j*\")\n",
    "for image in images:\n",
    "    filename = image\n",
    "    # load image from file\n",
    "    pixels = pyplot.imread(filename)\n",
    "    # create the detector, using default weights\n",
    "    detector = MTCNN()\n",
    "    # detect faces in the image\n",
    "    faces = detector.detect_faces(pixels)\n",
    "    # display faces on the original image\n",
    "    extract_faces(filename, faces)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process test set to extract faces.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dataset/test_set/Niyati/IMG_6664.jpg', 'dataset/test_set/Niyati/20150502_104836.jpeg', 'dataset/test_set/Niyati/20150516_223333.jpg', 'dataset/test_set/Niyati/DSC02597.jpeg', 'dataset/test_set/Niyati/IMG_8466.jpg', 'dataset/test_set/Niyati/20150516_223323.jpg', 'dataset/test_set/Niyati/IMG_0624.jpeg', 'dataset/test_set/Niyati/IMG_8464.jpg', 'dataset/test_set/Niyati/IMG_20150426_201701.jpg', 'dataset/test_set/Niyati/20171019_194210.jpeg', 'dataset/test_set/Niyati/IMG_7916.jpg', 'dataset/test_set/Niyati/IMG_1381.jpg', 'dataset/test_set/Niyati/20150507_202558.jpg', 'dataset/test_set/Niyati/IMG_8549.jpg', 'dataset/test_set/Niyati/IMG_0623.jpeg', 'dataset/test_set/Niyati/IMG_8429.jpg', 'dataset/test_set/Niyati/IMG_7915.jpg', 'dataset/test_set/Niyati/IMG_2129.jpg', 'dataset/test_set/Niyati/20150514_221720.jpg', 'dataset/test_set/Niyati/IMG_7456.jpg', 'dataset/test_set/Niyati/IMG_7536.jpg', 'dataset/test_set/Niyati/20150503_185608.jpg', 'dataset/test_set/Niyati/IMG_0115.jpeg', 'dataset/test_set/Niyati/IMG_5852.jpg', 'dataset/test_set/Niyati/20150501_162955.jpg', 'dataset/test_set/Niyati/IMG_6961.jpg', 'dataset/test_set/Niyati/IMG_7918.jpg', 'dataset/test_set/Niyati/IMG_6144.jpg', 'dataset/test_set/Niyati/IMG_7919.jpg', 'dataset/test_set/Niyati/20150501_162947.jpg', 'dataset/test_set/Niyati/IMG_8546.jpg', 'dataset/test_set/Niyati/20150502_131543.jpg', 'dataset/test_set/Niyati/IMG_2315.jpeg', 'dataset/test_set/Niyati/IMG_20150426_193927.jpg', 'dataset/test_set/Niyati/20150428_202010.jpg', 'dataset/test_set/Niyati/20150524_140354.jpg', 'dataset/test_set/Niyati/DSC02280.jpeg', 'dataset/test_set/Vanshika/IMG_3300.jpeg', 'dataset/test_set/Vanshika/IMG_1852.jpeg', 'dataset/test_set/Vanshika/IMG_0822.jpg', 'dataset/test_set/Vanshika/IMG_2993.jpeg', 'dataset/test_set/Vanshika/IMG_2213.jpeg', 'dataset/test_set/Vanshika/IMG_0990.jpg', 'dataset/test_set/Vanshika/IMG_3550.jpeg', 'dataset/test_set/Vanshika/IMG_2016.jpg', 'dataset/test_set/Vanshika/IMG_0946.jpg', 'dataset/test_set/Vanshika/IMG_9545.jpg', 'dataset/test_set/Vanshika/IMG_2015.jpg', 'dataset/test_set/Vanshika/20150815_124528.jpg', 'dataset/test_set/Vanshika/20150614_095727.jpg', 'dataset/test_set/Vanshika/IMG_1116.jpeg', 'dataset/test_set/Vanshika/20150608_082758.jpg', 'dataset/test_set/Vanshika/IMG_0822.jpeg', 'dataset/test_set/Vanshika/IMG_4031.jpeg', 'dataset/test_set/Vanshika/IMG_2074.jpeg', 'dataset/test_set/Vanshika/IMG_0111.jpg', 'dataset/test_set/Vanshika/IMG_3167.jpeg', 'dataset/test_set/Vanshika/20150615_083144_Richtone(HDR).jpg', 'dataset/test_set/Vanshika/20160616_185215.jpg', 'dataset/test_set/Vanshika/IMG_0733.jpg', 'dataset/test_set/Vanshika/20160519_105916.jpg', 'dataset/test_set/Vanshika/COLOR_POP.jpg', 'dataset/test_set/Vanshika/IMG_1832.jpg', 'dataset/test_set/Vanshika/20151004_202835.jpg', 'dataset/test_set/Vanshika/IMG_1013.jpg', 'dataset/test_set/Vanshika/IMG_0126.jpg', 'dataset/test_set/Vanshika/20150612_091141_009.jpg', 'dataset/test_set/Vanshika/IMG_4449.jpeg', 'dataset/test_set/Vanshika/IMG_2227.jpeg', 'dataset/test_set/Vanshika/IMG_9574.jpg', 'dataset/test_set/Vanshika/IMG_1116.jpg', 'dataset/test_set/Vanshika/IMG-20151007-WA0004.jpg', 'dataset/test_set/Vanshika/20150815_124533.jpg', 'dataset/test_set/Vanshika/IMG_1852.jpg', 'dataset/test_set/Vanshika/COLOR_POP(1).jpg', 'dataset/test_set/Vanshika/IMG_1853.jpg', 'dataset/test_set/Vanshika/IMG_3909.jpeg']\n"
     ]
    }
   ],
   "source": [
    "# do image extraction i.e., just extract faces to increase accuracy.. this will exclude clothing \n",
    "\n",
    "# extract and save each detected face in a photograph\n",
    "def extract_faces(filename, result_list):\n",
    "\t# load the image\n",
    "\tdata = pyplot.imread(filename)\n",
    "\t# plot each face as a subplot\n",
    "\tfor i in range(len(result_list)):\n",
    "\t\t# data = pyplot.imread(filename)\n",
    "\t\tconfidence = result_list[i]['confidence']        \n",
    "\t\t# get coordinates\n",
    "\t\tx1, y1, width, height = result_list[i]['box']\n",
    "\t\tx2, y2 = x1 + width, y1 + height\n",
    "\t\tif confidence >= 0.95:\n",
    "\t\t\t\tpyplot.imsave('output/'+ filename, data[y1:y2, x1:x2])        \n",
    "\n",
    "\n",
    "print(glob.glob(\"dataset/test_set/*/*.jp*\"))\n",
    "images = glob.glob(\"dataset/test_set/*/*.jp*\")\n",
    "for image in images:\n",
    "    filename = image\n",
    "    # load image from file\n",
    "    pixels = pyplot.imread(filename)\n",
    "    # create the detector, using default weights\n",
    "    detector = MTCNN()\n",
    "    # detect faces in the image\n",
    "    faces = detector.detect_faces(pixels)\n",
    "    # display faces on the original image\n",
    "    extract_faces(filename, faces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oxQxCBWyoGPE"
   },
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MvE-heJNo3GG"
   },
   "source": [
    "### Load the Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0koUcJMJpEBD"
   },
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True)\n",
    "training_set = train_datagen.flow_from_directory('output/dataset/training_set',\n",
    "                                                 target_size = (256, 256),\n",
    "                                                 batch_size = 10,\n",
    "                                                 class_mode = 'binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mrCMmGw9pHys"
   },
   "source": [
    "### Load the Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SH4WzfOhpKc3"
   },
   "outputs": [],
   "source": [
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "test_set = test_datagen.flow_from_directory('output/dataset/test_set',\n",
    "                                            target_size = (256, 256),\n",
    "                                            batch_size = 10,\n",
    "                                            class_mode = 'binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "af8O4l90gk7B"
   },
   "source": [
    "## Building the CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ces1gXY2lmoX"
   },
   "source": [
    "### Initialising the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SAUt4UMPlhLS"
   },
   "outputs": [],
   "source": [
    "cnn = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u5YJj_XMl5LF"
   },
   "source": [
    "### Step 1 - Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XPzPrMckl-hV"
   },
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=[256, 256, 3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tf87FpvxmNOJ"
   },
   "source": [
    "### Step 2 - Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ncpqPl69mOac"
   },
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xaTOgD8rm4mU"
   },
   "source": [
    "### Adding a second convolutional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i_-FZjn_m8gk"
   },
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu'))\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tmiEuvTunKfk"
   },
   "source": [
    "### Step 3 - Flattening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6AZeOGCvnNZn"
   },
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dAoSECOm203v"
   },
   "source": [
    "### Step 4 - Full Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8GtmUlLd26Nq"
   },
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(units=128, activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yTldFvbX28Na"
   },
   "source": [
    "### Step 5 - Output Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1p_Zj1Mc3Ko_"
   },
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D6XkI90snSDl"
   },
   "source": [
    "## Training the CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vfrFQACEnc6i"
   },
   "source": [
    "### Compiling the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NALksrNQpUlJ"
   },
   "outputs": [],
   "source": [
    "cnn.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ehS-v3MIpX2h"
   },
   "source": [
    "### Training the CNN on the Training set and evaluating it on the Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XUj1W4PJptta"
   },
   "outputs": [],
   "source": [
    "cnn.fit(x = training_set, validation_data = test_set, epochs = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U3PZasO0006Z"
   },
   "source": [
    "## Making predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "from matplotlib.patches import Rectangle\n",
    "from matplotlib.patches import Circle\n",
    "from mtcnn.mtcnn import MTCNN\n",
    "import glob\n",
    "\n",
    "def predict_person(filename):\n",
    "    model = cnn\n",
    "    import numpy as np\n",
    "    from keras.preprocessing import image\n",
    "    # N c\n",
    "    test_image = image.load_img(filename, target_size = (256, 256))\n",
    "    test_image = image.img_to_array(test_image)\n",
    "    # model was training with a batch of images, so have to expand dimensions\n",
    "    test_image = np.expand_dims(test_image, axis = 0)\n",
    "    # feature scaling i.e., normalization\n",
    "    result = model.predict(test_image/255.0)\n",
    "    # find which index corresponds to which classification class\n",
    "    # print(training_set.class_indices)\n",
    "    # access the first batch and then the first image\n",
    "    print(result[0][0])\n",
    "    if result[0][0] > 0.5:\n",
    "      prediction = 'Vanshika'\n",
    "    else:\n",
    "      prediction = 'Niyati'\n",
    "    print(prediction)\n",
    "\n",
    "# extract each face separately\n",
    "def extract_faces(filename, result_list):\n",
    "        # load the image\n",
    "        data = pyplot.imread(filename)\n",
    "        current_confidence = 0 \n",
    "        for i in range(len(result_list)):\n",
    "        # data = pyplot.imread(filename)\n",
    "            confidence = result_list[i]['confidence']\n",
    "            # this confidence comparison is done to ensure that if MTCNN identifies multiple faces then we take the one \n",
    "            # with the highest confidence.. our input will always have one person only in the image\n",
    "            if confidence > current_confidence:\n",
    "               # get coordinates\n",
    "                           x1, y1, width, height = result_list[i]['box']\n",
    "                           x2, y2 = x1 + width, y1 + height\n",
    "                           pyplot.imsave('tmp/'+filename, data[y1:y2, x1:x2])   \n",
    "                           pyplot.imshow(data[y1:y2, x1:x2])\n",
    "                           pyplot.show() \n",
    "                           current_confidence = confidence \n",
    "        predict_person('tmp/'+filename)                         \n",
    "                           \n",
    "images = glob.glob(\"dataset/guess/*.jp*\")\n",
    "for image in images:\n",
    "    print(\"********\")\n",
    "    filename = image\n",
    "    print(image)    \n",
    "    # load image from file\n",
    "    pixels = pyplot.imread(filename)    \n",
    "    pyplot.imshow(pixels)\n",
    "    pyplot.show()\n",
    "    # create the detector, using default weights\n",
    "    detector = MTCNN()\n",
    "    # detect faces in the image\n",
    "    faces = detector.detect_faces(pixels)\n",
    "    # display faces on the original image\n",
    "    extract_faces(filename, faces)\n",
    "    print(\"********\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-18 13:45:25.008533: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: imgClassification/assets\n"
     ]
    }
   ],
   "source": [
    "cnn.save('imgClassification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyON0YxX/oky4tPbqCLnFjWD",
   "collapsed_sections": [],
   "name": "convolutional_neural_network.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
